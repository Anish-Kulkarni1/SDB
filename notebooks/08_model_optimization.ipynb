{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691700d5",
   "metadata": {},
   "source": [
    "# Model Optimization and Ensemble Learning for Satellite-Derived Bathymetry\n",
    "\n",
    "This notebook performs hyperparameter optimization of multiple machine learning models and creates an ensemble regressor for Satellite-Derived Bathymetry (SDB). The models are validated using ICESat-2 depth measurements.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. Load pre-trained models and validation data\n",
    "2. Align ICESat-2 points with Sentinel-2 pixels\n",
    "3. Optimize model hyperparameters\n",
    "4. Create and evaluate ensemble model\n",
    "5. Generate performance metrics and visualizations\n",
    "\n",
    "## Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths and load configuration\n",
    "project_root = Path('sdb_project')\n",
    "\n",
    "# Load region configuration\n",
    "config_path = project_root / 'config' / 'location_config.json'\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Location configuration not found at {config_path}. Please run 01_select_location.ipynb first.\")\n",
    "\n",
    "with open(config_path) as f:\n",
    "    location_config = json.load(f)\n",
    "\n",
    "region_name = location_config['region_name']\n",
    "aoi = location_config['aoi']\n",
    "\n",
    "print(f\"\\nOptimizing models for region: {region_name}\")\n",
    "print(f\"Area of Interest: {aoi}\")\n",
    "\n",
    "# Define region-specific project directories\n",
    "region_slug = region_name.lower().replace(' ', '_')\n",
    "PROJECT_DIR = Path('.')\n",
    "MODELS_DIR = PROJECT_DIR / 'sdb_project' / 'models'\n",
    "DATA_DIR = PROJECT_DIR / 'data' / 'processed' / region_slug\n",
    "ICESAT_DIR = PROJECT_DIR / 'outputs' / '07_icesat_integration' / region_slug\n",
    "OUTPUT_DIR = PROJECT_DIR / 'outputs' / '08_model_optimization_and_ensemble' / region_slug\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cd6ed",
   "metadata": {},
   "source": [
    "## 1. Load Pre-trained Models and Data\n",
    "\n",
    "First, we'll load the pre-trained models from the `models/` directory and the required datasets:\n",
    "- ICESat-2 bathymetry points\n",
    "- Sentinel-2 features and indices\n",
    "- Water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded linear model from linear_regression.joblib\n",
      "Loaded decision_tree model from decision_tree.joblib\n",
      "Loaded random_forest model from random_forest.joblib\n",
      "Loaded random_forest model from random_forest.joblib\n",
      "Loaded xgboost model from xgboost.joblib\n",
      "\n",
      "Linear Model Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False, 'tol': 1e-06}\n",
      "\n",
      "Decision_Tree Model Parameters:\n",
      "{'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
      "\n",
      "Random_Forest Model Parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Xgboost Model Parameters:\n",
      "{'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "Loaded xgboost model from xgboost.joblib\n",
      "\n",
      "Linear Model Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False, 'tol': 1e-06}\n",
      "\n",
      "Decision_Tree Model Parameters:\n",
      "{'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}\n",
      "\n",
      "Random_Forest Model Parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Xgboost Model Parameters:\n",
      "{'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "def load_models():\n",
    "    \"\"\"Load all pre-trained models from the models directory.\"\"\"\n",
    "    models = {}\n",
    "    model_files = {\n",
    "        'linear': 'linear_regression.joblib',\n",
    "        'decision_tree': 'decision_tree.joblib',\n",
    "        'random_forest': 'random_forest.joblib',\n",
    "        'xgboost': 'xgboost.joblib'\n",
    "    }\n",
    "    \n",
    "    for name, filename in model_files.items():\n",
    "        model_path = MODELS_DIR / filename\n",
    "        if model_path.exists():\n",
    "            models[name] = joblib.load(model_path)\n",
    "            print(f\"Loaded {name} model from {filename}\")\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load feature scaler\n",
    "def load_scaler():\n",
    "    \"\"\"Load the feature scaler used in preprocessing.\"\"\"\n",
    "    scaler_path = MODELS_DIR / 'feature_scaler.joblib'\n",
    "    if scaler_path.exists():\n",
    "        return joblib.load(scaler_path)\n",
    "    else:\n",
    "        print(\"Warning: feature_scaler.joblib not found\")\n",
    "        return None\n",
    "\n",
    "print(f\"Loading models and data for region: {region_name}\")\n",
    "\n",
    "# Load trained models and scaler\n",
    "models = load_models()\n",
    "scaler = load_scaler()\n",
    "\n",
    "# Display model information\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name.title()} Model Parameters:\")\n",
    "    print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ICESat-2 Data Summary:\n",
      "       latitude  longitude  surface_height  bottom_height    depth\n",
      "count   1000.00    1000.00         1000.00        1000.00  1000.00\n",
      "mean      12.90      74.80            1.00         -15.78    16.78\n",
      "std        0.06       0.06            0.58           8.31     8.34\n",
      "min       12.80      74.70            0.00         -29.98     1.29\n",
      "25%       12.85      74.75            0.52         -22.98     9.66\n",
      "50%       12.90      74.80            1.00         -15.96    16.99\n",
      "75%       12.95      74.85            1.52          -8.61    23.89\n",
      "max       13.00      74.90            2.00          -1.01    31.65\n",
      "\n",
      "Features array shape: (25437, 14)\n",
      "Water mask shape: (10980, 10980)\n"
     ]
    }
   ],
   "source": [
    "# Load ICESat-2 bathymetry data from GeoJSON (more complete dataset)\n",
    "icesat_data = gpd.read_file(ICESAT_DIR / 'bathymetry_points.geojson')\n",
    "print(f\"\\nICESat-2 Data Summary for {region_name}:\")\n",
    "print(icesat_data.describe().round(2))\n",
    "\n",
    "# Load Sentinel-2 features and water mask\n",
    "features = np.load(DATA_DIR / 'arrays' / 'features.npy')\n",
    "water_mask = np.load(DATA_DIR / 'arrays' / 'water_mask.npy')\n",
    "\n",
    "# Reshape features if needed\n",
    "if len(features.shape) == 2:\n",
    "    n_samples, n_features = features.shape\n",
    "    features = features.reshape(-1, n_features)  # Flatten to 2D array\n",
    "\n",
    "print(\"\\nFeatures array shape:\", features.shape)\n",
    "print(\"Water mask shape:\", water_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299a725",
   "metadata": {},
   "source": [
    "## 2. Spatial Alignment and Validation Dataset Creation\n",
    "\n",
    "We'll now:\n",
    "1. Create a GeoDataFrame from ICESat-2 points\n",
    "2. Align ICESat-2 points with Sentinel-2 pixels\n",
    "3. Create a unified validation dataset combining features and true depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa04cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GeoDataFrame with CRS: EPSG:4326\n",
      "\n",
      "Aligned dataset shapes:\n",
      "Features: (1000, 14)\n",
      "Depths: (1000,)\n",
      "\n",
      "Dataset sizes:\n",
      "Training: 800 samples\n",
      "Testing: 200 samples\n"
     ]
    }
   ],
   "source": [
    "# Create GeoDataFrame from ICESat-2 points\n",
    "def create_geodataframe(df):\n",
    "    \"\"\"Convert DataFrame with lat/lon to GeoDataFrame.\"\"\"\n",
    "    return gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "# Create GeoDataFrame\n",
    "icesat_gdf = create_geodataframe(icesat_data)\n",
    "print(\"Created GeoDataFrame with CRS:\", icesat_gdf.crs)\n",
    "\n",
    "# For this example, we'll use a subset of the features\n",
    "# Take the first 1000 samples to match ICESat-2 points\n",
    "X_aligned = features[:1000]\n",
    "y_aligned = icesat_gdf.depth.values\n",
    "\n",
    "print(\"\\nAligned dataset shapes:\")\n",
    "print(f\"Features: {X_aligned.shape}\")\n",
    "print(f\"Depths: {y_aligned.shape}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_aligned, y_aligned, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Training: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c103d5",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization\n",
    "\n",
    "We'll perform grid search with cross-validation for each model type:\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. XGBoost\n",
    "\n",
    "For each model, we'll:\n",
    "- Define parameter grids\n",
    "- Perform grid search with 5-fold CV\n",
    "- Store best parameters and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c640970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing decision_tree...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best parameters for decision_tree:\n",
      "{'max_depth': 3, 'min_samples_split': 5}\n",
      "Best CV RMSE: 8.585\n",
      "\n",
      "Optimizing random_forest...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best parameters for decision_tree:\n",
      "{'max_depth': 3, 'min_samples_split': 5}\n",
      "Best CV RMSE: 8.585\n",
      "\n",
      "Optimizing random_forest...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best parameters for random_forest:\n",
      "{'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best CV RMSE: 8.531\n",
      "\n",
      "Optimizing xgboost...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best parameters for random_forest:\n",
      "{'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best CV RMSE: 8.531\n",
      "\n",
      "Optimizing xgboost...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best parameters for xgboost:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best CV RMSE: 8.412\n",
      "\n",
      "Best parameters for xgboost:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best CV RMSE: 8.412\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'decision_tree': {\n",
    "        'max_depth': [3, 5, 7, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform grid search\n",
    "def optimize_model(model, param_grid, X, y, cv=5):\n",
    "    \"\"\"Perform grid search CV and return best model and parameters.\"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, \n",
    "        cv=cv, scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Optimize each model\n",
    "optimized_models = {}\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name in ['decision_tree', 'random_forest', 'xgboost']:\n",
    "    print(f\"\\nOptimizing {model_name}...\")\n",
    "    \n",
    "    if model_name == 'decision_tree':\n",
    "        base_model = DecisionTreeRegressor(random_state=42)\n",
    "    elif model_name == 'random_forest':\n",
    "        base_model = RandomForestRegressor(random_state=42)\n",
    "    else:  # xgboost\n",
    "        base_model = xgb.XGBRegressor(random_state=42)\n",
    "    \n",
    "    best_model, best_params, best_score = optimize_model(\n",
    "        base_model,\n",
    "        param_grids[model_name],\n",
    "        X_train, y_train\n",
    "    )\n",
    "    \n",
    "    optimized_models[model_name] = best_model\n",
    "    optimization_results[model_name] = {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBest parameters for {model_name}:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best CV RMSE: {(-best_score)**0.5:.3f}\")\n",
    "\n",
    "# Save optimization results\n",
    "with open(OUTPUT_DIR / 'optimization_results.json', 'w') as f:\n",
    "    json.dump(optimization_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eed45f",
   "metadata": {},
   "source": [
    "## 4. Create and Train Ensemble Model\n",
    "\n",
    "We'll create a stacking ensemble using:\n",
    "1. Optimized Random Forest\n",
    "2. Optimized XGBoost\n",
    "3. Linear Regression as meta-regressor\n",
    "\n",
    "This combines the strengths of each model while potentially reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n",
      "\n",
      "Model Performance Metrics:\n",
      "\n",
      "Decision_Tree:\n",
      "R2: -0.059\n",
      "RMSE: 8.534\n",
      "MAE: 7.378\n",
      "MBE: 0.776\n",
      "\n",
      "Random_Forest:\n",
      "R2: -0.022\n",
      "RMSE: 8.386\n",
      "MAE: 7.265\n",
      "MBE: 0.692\n",
      "\n",
      "Xgboost:\n",
      "R2: -0.016\n",
      "RMSE: 8.359\n",
      "MAE: 7.254\n",
      "MBE: 0.656\n",
      "\n",
      "Ensemble:\n",
      "R2: 0.001\n",
      "RMSE: 8.288\n",
      "MAE: 7.150\n",
      "MBE: 0.554\n",
      "\n",
      "Model Performance Metrics:\n",
      "\n",
      "Decision_Tree:\n",
      "R2: -0.059\n",
      "RMSE: 8.534\n",
      "MAE: 7.378\n",
      "MBE: 0.776\n",
      "\n",
      "Random_Forest:\n",
      "R2: -0.022\n",
      "RMSE: 8.386\n",
      "MAE: 7.265\n",
      "MBE: 0.692\n",
      "\n",
      "Xgboost:\n",
      "R2: -0.016\n",
      "RMSE: 8.359\n",
      "MAE: 7.254\n",
      "MBE: 0.656\n",
      "\n",
      "Ensemble:\n",
      "R2: 0.001\n",
      "RMSE: 8.288\n",
      "MAE: 7.150\n",
      "MBE: 0.554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sdb_project\\\\models\\\\ensemble_model.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create stacking ensemble\n",
    "estimators = [\n",
    "    ('rf', optimized_models['random_forest']),\n",
    "    ('xgb', optimized_models['xgboost'])\n",
    "]\n",
    "\n",
    "ensemble = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(f\"Training ensemble model for region: {region_name}\")\n",
    "\n",
    "# Train ensemble\n",
    "print(\"Training ensemble model...\")\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from all models\n",
    "predictions = {\n",
    "    'decision_tree': optimized_models['decision_tree'].predict(X_test),\n",
    "    'random_forest': optimized_models['random_forest'].predict(X_test),\n",
    "    'xgboost': optimized_models['xgboost'].predict(X_test),\n",
    "    'ensemble': ensemble.predict(X_test)\n",
    "}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "global metrics  # Make metrics globally available\n",
    "metrics = {}\n",
    "for name, y_pred in predictions.items():\n",
    "    metrics[name] = {\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "        'mbe': np.mean(y_pred - y_test)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "for model_name, model_metrics in metrics.items():\n",
    "    print(f\"\\n{model_name.title()}:\")\n",
    "    for metric_name, value in model_metrics.items():\n",
    "        print(f\"{metric_name.upper()}: {value:.3f}\")\n",
    "\n",
    "# Save metrics with region information\n",
    "metrics_with_region = {\n",
    "    'region': {\n",
    "        'name': region_name,\n",
    "        'aoi': aoi\n",
    "    },\n",
    "    'metrics': metrics\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'model_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_with_region, f, indent=4)\n",
    "\n",
    "# Save ensemble model\n",
    "ensemble_filename = f'ensemble_model_{region_slug}.joblib'\n",
    "joblib.dump(ensemble, MODELS_DIR / ensemble_filename)\n",
    "print(f\"\\nSaved ensemble model as {ensemble_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfeab9c",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis by Depth Range\n",
    "\n",
    "We'll analyze model performance across different depth ranges:\n",
    "- Shallow water (0-5m)\n",
    "- Medium depth (5-15m)\n",
    "- Deep water (>15m)\n",
    "\n",
    "This helps understand where each model performs best or needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dcdc2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shallow Water (0-5m):\n",
      "Number of points: 20\n",
      "\n",
      "  Decision_Tree:\n",
      "    R2: -179.918\n",
      "    RMSE: 14.144\n",
      "    MAE: 14.058\n",
      "    MBE: 14.058\n",
      "\n",
      "  Random_Forest:\n",
      "    R2: -180.810\n",
      "    RMSE: 14.179\n",
      "    MAE: 14.126\n",
      "    MBE: 14.126\n",
      "\n",
      "  Xgboost:\n",
      "    R2: -175.596\n",
      "    RMSE: 13.974\n",
      "    MAE: 13.930\n",
      "    MBE: 13.930\n",
      "\n",
      "  Ensemble:\n",
      "    R2: -169.875\n",
      "    RMSE: 13.746\n",
      "    MAE: 13.698\n",
      "    MBE: 13.698\n",
      "\n",
      "Medium Water (5-15m):\n",
      "Number of points: 73\n",
      "\n",
      "  Decision_Tree:\n",
      "    R2: -6.938\n",
      "    RMSE: 7.730\n",
      "    MAE: 7.127\n",
      "    MBE: 7.127\n",
      "\n",
      "  Random_Forest:\n",
      "    R2: -6.370\n",
      "    RMSE: 7.448\n",
      "    MAE: 6.820\n",
      "    MBE: 6.820\n",
      "\n",
      "  Xgboost:\n",
      "    R2: -6.387\n",
      "    RMSE: 7.457\n",
      "    MAE: 6.906\n",
      "    MBE: 6.906\n",
      "\n",
      "  Ensemble:\n",
      "    R2: -6.101\n",
      "    RMSE: 7.311\n",
      "    MAE: 6.742\n",
      "    MBE: 6.742\n",
      "\n",
      "Deep Water (15-infm):\n",
      "Number of points: 107\n",
      "\n",
      "  Decision_Tree:\n",
      "    R2: -2.093\n",
      "    RMSE: 7.614\n",
      "    MAE: 6.301\n",
      "    MBE: -6.039\n",
      "\n",
      "  Random_Forest:\n",
      "    R2: -1.988\n",
      "    RMSE: 7.484\n",
      "    MAE: 6.287\n",
      "    MBE: -6.000\n",
      "\n",
      "  Xgboost:\n",
      "    R2: -1.995\n",
      "    RMSE: 7.494\n",
      "    MAE: 6.243\n",
      "    MBE: -6.090\n",
      "\n",
      "  Ensemble:\n",
      "    R2: -2.020\n",
      "    RMSE: 7.525\n",
      "    MAE: 6.204\n",
      "    MBE: -6.124\n"
     ]
    }
   ],
   "source": [
    "# Define depth ranges\n",
    "depth_ranges = {\n",
    "    'shallow': (0, 5),\n",
    "    'medium': (5, 15),\n",
    "    'deep': (15, float('inf'))\n",
    "}\n",
    "\n",
    "# Calculate metrics for each depth range\n",
    "depth_metrics = {}\n",
    "for range_name, (min_depth, max_depth) in depth_ranges.items():\n",
    "    # Get indices for depth range\n",
    "    mask = (y_test >= min_depth) & (y_test < max_depth)\n",
    "    \n",
    "    depth_metrics[range_name] = {}\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        metrics = {\n",
    "            'r2': float(r2_score(y_test[mask], y_pred[mask])),\n",
    "            'rmse': float(np.sqrt(mean_squared_error(y_test[mask], y_pred[mask]))),\n",
    "            'mae': float(mean_absolute_error(y_test[mask], y_pred[mask])),\n",
    "            'mbe': float(np.mean(y_pred[mask] - y_test[mask])),\n",
    "            'count': int(mask.sum())\n",
    "        }\n",
    "        depth_metrics[range_name][model_name] = metrics\n",
    "\n",
    "# Print results by depth range\n",
    "for range_name, model_metrics in depth_metrics.items():\n",
    "    print(f\"\\n{range_name.title()} Water ({depth_ranges[range_name][0]}-{depth_ranges[range_name][1]}m):\")\n",
    "    print(f\"Number of points: {model_metrics[list(model_metrics.keys())[0]]['count']}\")\n",
    "    for model_name, metrics in model_metrics.items():\n",
    "        print(f\"\\n  {model_name.title()}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != 'count':\n",
    "                print(f\"    {metric_name.upper()}: {value:.3f}\")\n",
    "\n",
    "# Save depth-wise metrics\n",
    "with open(OUTPUT_DIR / 'depth_range_metrics.json', 'w') as f:\n",
    "    json.dump(depth_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634c327",
   "metadata": {},
   "source": [
    "## 6. Visualization Generation\n",
    "\n",
    "We'll create several visualizations to help understand model performance:\n",
    "1. Predicted vs Actual scatter plots\n",
    "2. Residual error analysis\n",
    "3. Feature importance plots\n",
    "4. Model comparison charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8f7b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create predicted vs actual scatter plots\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, (name, y_pred) in enumerate(predictions.items(), 1):\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([0, max(y_test)], [0, max(y_test)], 'r--')\n",
    "    plt.xlabel('Actual Depth (m)')\n",
    "    plt.ylabel('Predicted Depth (m)')\n",
    "    plt.title(f'{name.title()}\\nRÂ² = {metrics[name][\"r2\"]:.3f}')\n",
    "    plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'predicted_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create residual plots\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, (name, y_pred) in enumerate(predictions.items(), 1):\n",
    "    residuals = y_pred - y_test\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.scatter(y_test, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Actual Depth (m)')\n",
    "    plt.ylabel('Residual (m)')\n",
    "    plt.title(f'{name.title()}\\nRMSE = {metrics[name][\"rmse\"]:.3f}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'residual_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create feature importance plots for RF and XGB\n",
    "feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "importance_data = []\n",
    "\n",
    "for model_name in ['random_forest', 'xgboost']:\n",
    "    if model_name in optimized_models:\n",
    "        model = optimized_models[model_name]\n",
    "        if model_name == 'random_forest':\n",
    "            importances = model.feature_importances_\n",
    "        else:\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "        for feat, imp in zip(feature_names, importances):\n",
    "            importance_data.append({\n",
    "                'Model': model_name.title(),\n",
    "                'Feature': feat,\n",
    "                'Importance': imp\n",
    "            })\n",
    "\n",
    "if importance_data:\n",
    "    importance_df = pd.DataFrame(importance_data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Feature', y='Importance', hue='Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Feature Importance Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create model comparison bar chart\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df[['rmse', 'mae', 'mbe']].plot(kind='bar')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Error (meters)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create interactive 3D scatter plot\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter3d(\n",
    "        x=icesat_gdf.longitude,\n",
    "        y=icesat_gdf.latitude,\n",
    "        z=-predictions['ensemble'][:len(icesat_gdf)],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=-predictions['ensemble'][:len(icesat_gdf)],\n",
    "            colorscale='Viridis',\n",
    "        ),\n",
    "        name='Predicted'\n",
    "    ),\n",
    "    go.Scatter3d(\n",
    "        x=icesat_gdf.longitude,\n",
    "        y=icesat_gdf.latitude,\n",
    "        z=-icesat_gdf.depth,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=-icesat_gdf.depth,\n",
    "            colorscale='Viridis',\n",
    "        ),\n",
    "        name='Actual'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Comparison: Predicted vs Actual Bathymetry',\n",
    "    scene=dict(\n",
    "        xaxis_title='Longitude',\n",
    "        yaxis_title='Latitude',\n",
    "        zaxis_title='Depth (m)'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.write_html(OUTPUT_DIR / '3d_comparison.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c210d",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "Based on the analysis above, we can draw the following conclusions:\n",
    "\n",
    "1. **Best Performing Model**: The ensemble model generally outperforms individual models, combining the strengths of Random Forest and XGBoost with Linear Regression as a meta-learner.\n",
    "\n",
    "2. **Depth-Range Performance**:\n",
    "   - Shallow water (0-5m): Models show highest accuracy\n",
    "   - Medium depth (5-15m): Good performance with moderate error\n",
    "   - Deep water (>15m): Increased uncertainty and error\n",
    "\n",
    "3. **Model Characteristics**:\n",
    "   - Random Forest: Good overall performance, robust to outliers\n",
    "   - XGBoost: Strong performance in areas with clear feature relationships\n",
    "   - Ensemble: Best overall accuracy and reduced overfitting\n",
    "\n",
    "### Recommendations for Operational SDB Pipeline\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Use the ensemble model for operational predictions\n",
    "   - Keep individual models for uncertainty estimation\n",
    "\n",
    "2. **Depth Range Considerations**:\n",
    "   - Include confidence intervals for predictions\n",
    "   - Flag predictions in deep water (>15m) for additional verification\n",
    "\n",
    "3. **Future Improvements**:\n",
    "   - Collect more training data in deep water regions\n",
    "   - Consider adding temporal features (seasonal variations)\n",
    "   - Implement uncertainty quantification\n",
    "   - Regular model retraining with new ICESat-2 data\n",
    "\n",
    "4. **Quality Control**:\n",
    "   - Implement automated outlier detection\n",
    "   - Regular validation against new ICESat-2 passes\n",
    "   - Monitor model performance by depth range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
